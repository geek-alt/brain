Multimodal RAG AI BackendThis project provides a backend service for a personalized AI learning partner. It uses a Retrieval-Augmented Generation (RAG) architecture to answer questions based on user-uploaded documents.FeaturesDocument Upload: Upload PDF files to build a personalized knowledge base.Vectorization: Documents are automatically chunked, embedded, and stored in a ChromaDB vector store.RAG-based Q&A: Ask questions that are answered by a local LLM (via Ollama) using context retrieved from your documents.Personalization: Tailor the AI's responses by specifying the model, explanation level, and language.Dockerized: Comes with a Dockerfile for easy setup and deployment.How It WorksUpload: You send a PDF file to the /upload/ endpoint.Process: The backend extracts text, splits it into smaller chunks, and uses a SentenceTransformer model to convert each chunk into a numerical vector (embedding).Store: These embeddings are stored in a ChromaDB database, which is persisted on disk.Query: You send a question to the /query/ endpoint.Retrieve: The backend embeds your question and uses ChromaDB to find the most semantically similar text chunks from your documents.Generate: The question and the retrieved text chunks are combined into a detailed prompt and sent to a local LLM (like Llama 3) through Ollama. The LLM generates an answer based on the provided context.Getting StartedPrerequisitesDocker: You must have Docker installed and running on your system.Ollama: You need to have Ollama installed and have pulled a model. We recommend starting with Llama 3.ollama pull llama3
Running the BackendSave the Files: Save the main.py, requirements.txt, and Dockerfile into a new project directory.Build the Docker Image: Open a terminal in your project directory and run the following command. This will build the Docker image for our application.docker build -t rag-ai-backend .
Run the Docker Container: Once the image is built, run the container with this command. This starts the FastAPI server.docker run -p 8000:8000 --name rag-backend-container rag-ai-backend
-p 8000:8000: Maps port 8000 on your host machine to port 8000 in the container.--name rag-backend-container: Gives the container a memorable name.Verify It's Running: Open your web browser and navigate to http://localhost:8000. You should see the message: {"message":"Welcome to the RAG AI Backend..."}. You can also see the interactive API documentation at http://localhost:8000/docs.How to Connect Your FrontendYour React frontend can now make requests to this backend:To upload a file: Send a POST request with multipart/form-data to http://localhost:8000/upload/.To ask a question: Send a POST request with a JSON body to http://localhost:8000/query/.Example JSON for the /query/ endpoint:{
  "query": "What was the significance of the French Revolution in the history of mathematics?",
  "model": "llama3",
  "explanation_level": "expert",
  "language": "English"
}
This backend provides a solid foundation for your vision. You can extend it by adding support for more file types, implementing user accounts for separate knowledge bases, and refining the personalization logic.